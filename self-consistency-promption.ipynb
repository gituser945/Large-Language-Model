{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from operator import itemgetter\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from IPython.display import display,Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a question about apple quantities and usage.\n",
    "question = \"\"\"The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?\"\"\"\n",
    "\n",
    "# Provide context for answering questions, emphasizing detailed math and reasoning.\n",
    "context = \"\"\"Answer questions showing the full math and reasoning. Follow the pattern in the example.\"\"\"\n",
    "\n",
    "# Example problem-solving scenario involving arithmetic calculations.\n",
    "one_shot_example = \"\"\"Example \n",
    "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he \n",
    "have now? \n",
    "\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Q: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a planning pipeline using PromptTemplate, ChatOpenAI, StrOutputParser, and RunnablePassthrough.\n",
    "planner = (\n",
    "    \n",
    "    # Combine context and example with an input placeholder for dynamic prompt creation.\n",
    "    PromptTemplate.from_template(context + one_shot_example + \" {input}\")\n",
    "    \n",
    "    # Process the combined template through ChatOpenAI for text generation.\n",
    "    | ChatOpenAI()\n",
    "    \n",
    "    # Parse the generated text into structured data.\n",
    "    | StrOutputParser()\n",
    "    \n",
    "    # Wrap the parsed output in a RunnablePassthrough object for further processing.\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a structured response using PromptTemplate, ChatOpenAI, and StrOutputParser.\n",
    "answer_1 = (\n",
    "    \n",
    "    # Start with a base response template followed by \"A: \".\n",
    "    PromptTemplate.from_template(\"{base_response} A: \")\n",
    "    \n",
    "    # Use ChatOpenAI with temperature set to 0 for deterministic output.\n",
    "    | ChatOpenAI(temperature=0)\n",
    "    \n",
    "    # Parse the generated text into structured data.\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a structured response using PromptTemplate, ChatOpenAI, and StrOutputParser.\n",
    "answer_2 = (\n",
    "    \n",
    "    # Start with a base response template followed by \"A: \".\n",
    "    PromptTemplate.from_template(\"{base_response} A: \")\n",
    "    \n",
    "    # Use ChatOpenAI with temperature set to 0 for deterministic output.\n",
    "    | ChatOpenAI(temperature=0.1)\n",
    "    \n",
    "    # Parse the generated text into structured data.\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a structured response using PromptTemplate, ChatOpenAI, and StrOutputParser.\n",
    "answer_3 = (\n",
    "    \n",
    "    # Start with a base response template followed by \"A: \".\n",
    "    PromptTemplate.from_template(\"{base_response} A: \")\n",
    "    \n",
    "    # Use ChatOpenAI with temperature set to 0 for deterministic output.\n",
    "    | ChatOpenAI(temperature=0.7)\n",
    "    \n",
    "    # Parse the generated text into structured data.\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final responder setup for formatting and displaying results in markdown.\n",
    "final_responder = (\n",
    "    \n",
    "    # Template for outputting results in markdown format.\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Output all the final results in this markdown format: Result 1: {results_1} \\n Result 2:{results_2} \\n Result 3: \n",
    "        {results_3}\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Process the template through ChatOpenAI for text generation.\n",
    "    | ChatOpenAI()\n",
    "    \n",
    "    # Parse the generated text into structured data.\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain together components for a comprehensive response process.\n",
    "chain = (\n",
    "    \n",
    "    # Initial planning stage.\n",
    "    planner\n",
    "    \n",
    "    # Map results and original response to specific variables.\n",
    "    | {\n",
    "        \"results_1\": answer_1,\n",
    "        \"results_2\": answer_2,\n",
    "        \"results_3\": answer_3,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    \n",
    "    # Finalize the response with formatting and display.\n",
    "    | final_responder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Result 1: 9 apples. \n",
       "\n",
       "Result 2: The cafeteria had 9 apples left after lunch. \n",
       "\n",
       "Result 3: \n",
       "        The cafeteria had 9 apples left after lunch."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Invoke the response chain with a given input question.\n",
    "answers = chain.invoke({\"input\": question})\n",
    "\n",
    "# Display the generated answers in Markdown format.\n",
    "display(Markdown(answers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
